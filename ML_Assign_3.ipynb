{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c8f4a6c",
   "metadata": {},
   "source": [
    "1. Implement Linear Regression and calculate sum of residual error on the following\n",
    "Datasets.\n",
    "x = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "y = [1, 3, 2, 5, 7, 8, 8, 9, 10, 12]\n",
    " Compute the regression coefficients using analytic formulation and calculate Sum\n",
    "Squared Error (SSE) and R 2 value.\n",
    " Implement gradient descent (both Full-batch and Stochastic with stopping\n",
    "criteria) on Least Mean Square loss formulation to compute the coefficients of\n",
    "regression matrix and compare the results using performance measures such as R 2\n",
    "SSE etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "344fb940",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analytic solution:\n",
      "SSE: 5.624242424242423\n",
      "R squared: 0.952538038613988\n",
      "\n",
      "Full-batch Gradient Descent:\n",
      "SSE: 5.624278989977716\n",
      "R squared: 0.9525377300423822\n",
      "\n",
      "Stochastic Gradient Descent:\n",
      "SSE: 7.575559791810393\n",
      "R squared: 0.9360712253855663\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "x = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
    "y = np.array([1, 3, 2, 5, 7, 8, 8, 9, 10, 12])\n",
    "\n",
    "def analytic_lr(x, y):\n",
    "    n = len(x)\n",
    "    x_mean = np.mean(x)\n",
    "    y_mean = np.mean(y)\n",
    "    \n",
    "    num = np.sum((x - x_mean) * (y - y_mean))\n",
    "    denom = np.sum((x - x_mean) ** 2)\n",
    "    m = num / denom\n",
    "    c = y_mean - m * x_mean\n",
    "    \n",
    "    y_pred = m * x + c\n",
    "    sse = np.sum((y - y_pred) ** 2)\n",
    "    \n",
    "    ss_total = np.sum((y - y_mean) ** 2)\n",
    "    r_sq = 1 - (sse / ss_total)\n",
    "    \n",
    "    return sse, r_sq\n",
    "\n",
    "def full_gd_lr(x, y, lr, iterations):\n",
    "    n = len(x)\n",
    "    m = 0\n",
    "    c = 0\n",
    "    \n",
    "    for _ in range(iterations):\n",
    "        y_pred = m * x + c\n",
    "        dm = (-2/n) * np.sum(x * (y - y_pred))\n",
    "        dc = (-2/n) * np.sum(y - y_pred)\n",
    "        \n",
    "        m -= lr * dm\n",
    "        c -= lr * dc\n",
    "    \n",
    "    y_pred_final = m * x + c\n",
    "    sse = np.sum((y - y_pred_final) ** 2)\n",
    "    \n",
    "    y_mean = np.mean(y)\n",
    "    ss_total = np.sum((y - y_mean) ** 2)\n",
    "    r_sq = 1 - (sse / ss_total)\n",
    "    \n",
    "    return sse, r_sq\n",
    "\n",
    "def stochastic_gd_lr(x, y, lr, iterations):\n",
    "    n = len(x)\n",
    "    m = 0\n",
    "    c = 0\n",
    "    \n",
    "    for _ in range(iterations):\n",
    "        for i in range(n):\n",
    "            y_pred = m * x[i] + c\n",
    "            dm = (-2) * x[i] * (y[i] - y_pred)\n",
    "            dc = (-2) * (y[i] - y_pred)\n",
    "            \n",
    "            m -= lr * dm\n",
    "            c -= lr * dc\n",
    "    \n",
    "    y_pred_final = m * x + c\n",
    "    sse = np.sum((y - y_pred_final) ** 2)\n",
    "    \n",
    "    y_mean = np.mean(y)\n",
    "    ss_total = np.sum((y - y_mean) ** 2)\n",
    "    r_sq = 1 - (sse / ss_total)\n",
    "    \n",
    "    return sse, r_sq\n",
    "\n",
    "lr = 0.01\n",
    "iterations = 1000\n",
    "lr_stochastic = 0.01\n",
    "iterations_stochastic = 100\n",
    "\n",
    "sse_analytic, r_sq_analytic = analytic_lr(x, y)\n",
    "sse_gd_full, r_sq_gd_full = full_gd_lr(x, y, lr, iterations)\n",
    "sse_gd_stochastic, r_sq_gd_stochastic = stochastic_gd_lr(x, y, lr_stochastic, iterations_stochastic)\n",
    "\n",
    "print(\"Analytic solution:\")\n",
    "print(\"SSE:\", sse_analytic)\n",
    "print(\"R squared:\", r_sq_analytic)\n",
    "\n",
    "print(\"\\nFull-batch Gradient Descent:\")\n",
    "print(\"SSE:\", sse_gd_full)\n",
    "print(\"R squared:\", r_sq_gd_full)\n",
    "\n",
    "print(\"\\nStochastic Gradient Descent:\")\n",
    "print(\"SSE:\", sse_gd_stochastic)\n",
    "print(\"R squared:\", r_sq_gd_stochastic)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2612278b",
   "metadata": {},
   "source": [
    "2. Download Boston Housing Rate Dataset. Analyse the input attributes and find out the\n",
    "attribute that best follow the linear relationship with the output price. Implement both the\n",
    "analytic formulation and gradient descent (Full-batch, stochastic) on LMS loss\n",
    "formulation to compute the coefficients of regression matrix and compare the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6d588221",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best attribute: LSTAT\n",
      "Analytic coefficients: [34.23579926 -0.93006897]\n",
      "Gradient Descent (Full-batch) coefficients: [1.09075655]\n",
      "Stochastic Gradient Descent coefficients: [0.89061244]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('HousingData.csv', skiprows=1, header=None, names=['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV'], na_values='NA')\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "def lr_analytic(X, y):\n",
    "    X = np.column_stack((np.ones(len(X)), X))\n",
    "    coeffs = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y)\n",
    "    return coeffs\n",
    "\n",
    "def lr_gd(X, y, lr, num_iter):\n",
    "    n_s, n_f = X.shape\n",
    "    coeffs = np.zeros(n_f)\n",
    "    \n",
    "    for _ in range(num_iter):\n",
    "        error = np.dot(X, coeffs) - y\n",
    "        gradient = np.dot(X.T, error) / n_s\n",
    "        coeffs -= lr * gradient\n",
    "    \n",
    "    return coeffs\n",
    "\n",
    "def lr_sgd(X, y, lr, num_iter):\n",
    "    n_s, n_f = X.shape\n",
    "    coeffs = np.zeros(n_f)\n",
    "    \n",
    "    for _ in range(num_iter):\n",
    "        for i in range(n_s):\n",
    "            error = np.dot(X[i], coeffs) - y[i]\n",
    "            gradient = X[i] * error\n",
    "            coeffs -= lr * gradient\n",
    "    \n",
    "    return coeffs\n",
    "\n",
    "corr = df.corr()['MEDV'].abs().sort_values(ascending=False)\n",
    "best_attr = corr.index[1]\n",
    "\n",
    "X_data = df[best_attr].values.reshape(-1, 1)\n",
    "y_data = df['MEDV'].values\n",
    "\n",
    "lr = 0.0001\n",
    "num_iter = 1000\n",
    "\n",
    "coeff_analytic = lr_analytic(X_data, y_data)\n",
    "coeff_gd = lr_gd(X_data, y_data, lr, num_iter)\n",
    "coeff_sgd = lr_sgd(X_data, y_data, lr, num_iter)\n",
    "\n",
    "print(\"Best attribute:\", best_attr)\n",
    "print(\"Analytic coefficients:\", coeff_analytic)\n",
    "print(\"Gradient Descent (Full-batch) coefficients:\", coeff_gd)\n",
    "print(\"Stochastic Gradient Descent coefficients:\", coeff_sgd)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
